{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DL Workflow Fundementals :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: \n",
      " 2.5.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Torch version: \\n\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is anything, right?\n",
    "* Excel spreadsheet\n",
    "* Images\n",
    "* Videos\n",
    "* Audio\n",
    "* Text\n",
    "* DNA\n",
    "* other stuff\n",
    "\n",
    "ML is:\n",
    "1. Data into numerical rep/encoding\n",
    "2. Build a model to learn the patterns of that numerical rep/patterns/features/weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Into to Linear Regression\n",
    "\n",
    "We will use Linear Regression to create a straight line with *known* **parameters**\n",
    "Formula\n",
    "\n",
    "Y = a + (b*X)\n",
    "Where:\n",
    "- Y: Dependent var\n",
    "- X: Explanatory var\n",
    "- b: slope of the line\n",
    "- a: Intercept : y when X=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known params\n",
    "\n",
    "b = weight = 0.7 # -> b\n",
    "a = bias = 0.3 # -> a\n",
    "#you can chage these values as much as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "\n",
    "y = a + (b*X) #same as in the formula from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10], y[:10]\n",
    "# THis is the generated data, and now we need the model to figure out the relationship between the X and the Y,\n",
    "#we know cuz we made the data and the Formula, but the model won't know them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization: The ability of an ML Model to perform well on data it hasn't seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training Set 60-80% of the dataset - **always create**\n",
    "- Testing Set **often create**\n",
    "- Validation Set 10-20% **always create**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_split: \n",
      " 40 \n",
      "\n",
      "=================================\n",
      "X_train set length: \n",
      " 40 \n",
      "\n",
      "X_train set length: \n",
      " 10 \n",
      "\n",
      "y_train set length: \n",
      " 40 \n",
      "\n",
      "y_train set length: \n",
      " 10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_split_value = int(0.8 * len(X)) # -> 40\n",
    "print(\"train_split: \\n\", train_split_value, \"\\n\")\n",
    "\n",
    "# you may just put 40 here\n",
    "X_train = X[:train_split_value]\n",
    "y_train = y[:train_split_value]\n",
    "\n",
    "# and here we are putting the remaining of 80 split into 2, which is 20/2\n",
    "X_test = X[train_split_value:]\n",
    "y_test = y[train_split_value:]\n",
    "\n",
    "print(\"=================================\")\n",
    "print(\"X_train set length: \\n\", len(X_train),\"\\n\")\n",
    "print(\"X_train set length: \\n\", len(X_test),\"\\n\")\n",
    "print(\"y_train set length: \\n\", len(y_train),\"\\n\")\n",
    "print(\"y_train set length: \\n\", len(y_test),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a good way to represent this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions( \n",
    "    train_data = X_train,\n",
    "    train_labels = y_train,\n",
    "    test_data = X_test,\n",
    "    test_labels = y_test,\n",
    "    predictions = None\n",
    "):\n",
    "    \"\"\"Plots training data in blue, test data and compares predictions\"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")\n",
    "    \n",
    "    # Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", label=\"Test data\")\n",
    "    \n",
    "    if predictions is not None:\n",
    "        # Plot predictions in red\n",
    "        plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n",
    "    # Show legend\n",
    "    plt.legend(prop={\"size\":14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the function now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions() #since the inputs are hardcoded, we won't need pass them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to make  model to predict red dots over the green dots\n",
    "\n",
    "all of this is synthetic and simulative data, with a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a Lin Reg Model, it needs a parameters of `a` and `b`, before going into this section, get familiar with OOP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model behaviour:\n",
    "* Start with random values\n",
    "* Look at training data and adjust the `random values` to better represent the ideal values\n",
    "\n",
    "How does it adjust?\n",
    "2 Algo:\n",
    "1. Gradient Descent: This is for getting the least distance and most effecient one\n",
    "2. BackPropogation: This is to adjust the weights of our NN nodes to improve it, getting the avg of sum of all needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #making random numbers\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        # calculate and adjust the randomness using a Gradient Dessent Algo\n",
    "        # هذا هو الدالة اللي تشتغل على الموضوع، دالة فورورد\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        return self.weights * x + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets make an instance and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list:\n",
      " [Parameter containing:\n",
      "tensor([0.3367], requires_grad=True), Parameter containing:\n",
      "tensor([0.1288], requires_grad=True)]\n",
      "==========================================\n",
      "Parameters:\n",
      "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model instance of the Linear Reg Model\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check params\n",
    "print(f\"list:\\n {list(model_0.parameters())}\")\n",
    "print(\"==========================================\")\n",
    "print(f\"Parameters:\\n{model_0.state_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check model prection power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Predictions: \n",
      " tensor([[0.3982],\n",
      "        [0.4049],\n",
      "        [0.4116],\n",
      "        [0.4184],\n",
      "        [0.4251],\n",
      "        [0.4318],\n",
      "        [0.4386],\n",
      "        [0.4453],\n",
      "        [0.4520],\n",
      "        [0.4588]])\n",
      "=====================================\n",
      "Y Test: \n",
      "tensor([[0.8600],\n",
      "        [0.8740],\n",
      "        [0.8880],\n",
      "        [0.9020],\n",
      "        [0.9160],\n",
      "        [0.9300],\n",
      "        [0.9440],\n",
      "        [0.9580],\n",
      "        [0.9720],\n",
      "        [0.9860]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "    \n",
    "print(f\"Y Predictions: \\n {y_preds}\")\n",
    "print(\"=====================================\")\n",
    "print(f\"Y Test: \\n{y_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions = y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try plotting these different Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red dots are far from the green dots, that is becs of the random seed we made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Loss Function** : Measure how wrong your model's predictions are to the ideal outputs, lower loss is better :)\n",
    "\n",
    "- **Optimizer**: Takes the loss and adjusts the models params to improve the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an Optimmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(params = model_0.parameters(),\n",
    "                        lr = 0.01) #learning rate = most important hyper parameter possibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithim:\n",
    "0. Loop through the data\n",
    "1. forward pass to make the predictions\n",
    "2. calculate the loss\n",
    "3. Optimizer zero grad\n",
    "4. Loss Backward - back propogation -\n",
    "5. Optimizer step - gradient descent -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 0 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 10 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 20 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 30 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 40 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 50 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 60 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 70 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 80 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Epoch: 90 | Loss: 0.0025885067880153656 | Test Loss: 0.008447891101241112\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n",
      "Y predictions is: tensor([[0.2993],\n",
      "        [0.3132],\n",
      "        [0.3271],\n",
      "        [0.3410],\n",
      "        [0.3549],\n",
      "        [0.3688],\n",
      "        [0.3827],\n",
      "        [0.3966],\n",
      "        [0.4105],\n",
      "        [0.4244],\n",
      "        [0.4383],\n",
      "        [0.4522],\n",
      "        [0.4661],\n",
      "        [0.4800],\n",
      "        [0.4939],\n",
      "        [0.5078],\n",
      "        [0.5218],\n",
      "        [0.5357],\n",
      "        [0.5496],\n",
      "        [0.5635],\n",
      "        [0.5774],\n",
      "        [0.5913],\n",
      "        [0.6052],\n",
      "        [0.6191],\n",
      "        [0.6330],\n",
      "        [0.6469],\n",
      "        [0.6608],\n",
      "        [0.6747],\n",
      "        [0.6886],\n",
      "        [0.7025],\n",
      "        [0.7164],\n",
      "        [0.7303],\n",
      "        [0.7442],\n",
      "        [0.7581],\n",
      "        [0.7720],\n",
      "        [0.7859],\n",
      "        [0.7998],\n",
      "        [0.8137],\n",
      "        [0.8276],\n",
      "        [0.8415]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.0025885067880153656\n",
      "Y predictions is: tensor([[0.3093],\n",
      "        [0.3233],\n",
      "        [0.3373],\n",
      "        [0.3513],\n",
      "        [0.3652],\n",
      "        [0.3792],\n",
      "        [0.3932],\n",
      "        [0.4072],\n",
      "        [0.4212],\n",
      "        [0.4351],\n",
      "        [0.4491],\n",
      "        [0.4631],\n",
      "        [0.4771],\n",
      "        [0.4911],\n",
      "        [0.5050],\n",
      "        [0.5190],\n",
      "        [0.5330],\n",
      "        [0.5470],\n",
      "        [0.5610],\n",
      "        [0.5749],\n",
      "        [0.5889],\n",
      "        [0.6029],\n",
      "        [0.6169],\n",
      "        [0.6309],\n",
      "        [0.6448],\n",
      "        [0.6588],\n",
      "        [0.6728],\n",
      "        [0.6868],\n",
      "        [0.7008],\n",
      "        [0.7147],\n",
      "        [0.7287],\n",
      "        [0.7427],\n",
      "        [0.7567],\n",
      "        [0.7707],\n",
      "        [0.7847],\n",
      "        [0.7986],\n",
      "        [0.8126],\n",
      "        [0.8266],\n",
      "        [0.8406],\n",
      "        [0.8546]], grad_fn=<AddBackward0>)\n",
      "Loss is: 0.008932482451200485\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# One loop over the data\n",
    "epochs = 100\n",
    "\n",
    "#0. Loop\n",
    "for epoch in range(epochs):\n",
    "    # Set model to training mode\n",
    "    model_0.train()\n",
    "    \n",
    "    #1. Forward pass\n",
    "    y_preds = model_0(X_train)\n",
    "    print(f\"Y predictions is: {y_preds}\")\n",
    "    #2.Calc Loss\n",
    "    loss = loss_fn(y_preds, y_train)\n",
    "    print(f\"Loss is: {loss}\")\n",
    "    #3. Optimizer 0 grad\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    #4. Backprob on params\n",
    "    loss.backward()\n",
    "    \n",
    "    #5. Do gradient descent\n",
    "    optim.step()\n",
    "    \n",
    "    model_0.eval() #turns off gradient tracking\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_0(X_test)\n",
    "        \n",
    "        \n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the params after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3406])), ('bias', tensor([0.1388]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAH5CAYAAABJUkuHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABO0UlEQVR4nO3dCXhU5dn/8TsLqxCoIlsIpIIoVgRZRUSIRWPltUFLoaQC4lar8k9Ea8EFFEVsXRoKVHytFqsFURY3eOOCREGwtCAtyqJCgBBZqxBA2ZLzv+5neuLMZCaZSTKZOWe+n+uaTs6ZM2dO4oTOned57l+CZVmWAAAAAICLJEb7AgAAAACgtlHoAAAAAHAdCh0AAAAArkOhAwAAAMB1KHQAAAAAuA6FDgAAAADXodABAAAA4DrJ4gBlZWXy1VdfSdOmTSUhISHalwMAAAAgSjQG9PDhw9K2bVtJTEx0dqGjRU5aWlq0LwMAAABAjCgqKpJ27do5u9DRkRz7m0lJSYn25QAAAACIkpKSEjMIYtcIji507OlqWuRQ6AAAAABIqGJJC80IAAAAALgOhQ4AAAAA16HQAQAAAOA6FDoAAAAAXIdCBwAAAIDrUOgAAAAAcJ2w20t/+OGH8vjjj8vatWtl9+7dsnjxYhk6dGilzykoKJDx48fLZ599Znpe33///XL99ddLJJ08eVJKS0sj+hpArKpXr54kJSVF+zIAAACcU+gcPXpUunXrJjfccINce+21VR5fWFgoQ4YMkVtvvVX+9re/ybJly+Smm26SNm3aSGZmpkQiQOjAgQNy/PjxWj834KS+8s2aNZPWrVtX2WMeAADAjcIudH7yk5+YW6hmz54tP/zhD+XJJ5802126dJGVK1fKH/7wh1ovdLTIKS4uliZNmkiLFi3MX7X5kId4Y1mW+YPE/v37pVGjRtK8efNoXxIAAEDsFzrhWr16tQwePNhnnxY4ubm5QZ+jozHeIzJawIRCR3K0yGnXrh0FDuKaFjj6O7Rv3z4zssPvAwAAiDcRb0awZ88eadWqlc8+3dbi5bvvvgv4nGnTppkPZ/ZN1/WEsiZHP9jxoQ7wSElJMevUWKsGAADiUUx2XZs4caIcOnSo/FZUVFTlc+wPczpdDYBIcrJnwPbUqVPRvhQAAAD3TV3TxdB79+712afb+tdmnV4TSIMGDcytOhjNATz4XQAAAPEs4oVOv379ZOnSpT773n33XbMfAAAAQOwqLSuVFTtXyO7Du6VN0zYyoP0ASUpMcmehc+TIEfnyyy992kevX79eTj/9dGnfvr2Zdqadz/7617+ax7Wt9MyZM+Wee+4xLanff/99eeWVV2TJkiW1+50AAAAAqDWLNi2SnPwc2VWyq3xfu5R2Mv3K6XJtl6pjZhy3Ruef//ynXHjhheamNAhUv540aZLZ1hDRnTt3lh+vraW1qNFRHM3f0TbTf/7znyOSoQMAAACg5rTIGfbKMJ8iRxWXFJv9+nisS7A0dCPGaYc27aamjQl0bU8gx44dM6NLWlg1bNiwzq8x3teCDBw4UAoKCqp9Dn1uRkaGTJ48WR588EGJdenp6eZ++/btEqv4nQAAANWdrpY+Pb1CkWNLkAQzslOYUxiVaWyh1AYx23UN1Ss2wrkh+gYNGsR/CwAAEHNW7FxRXuQklokMLBT5xQbPvW5bYklRSZE5Lq6bEaBu6EiIv7y8PFPpBnqsNm3atEkaN25co3P06dPHnKdFixa1dl0AAAAInzYeUNdsFJmeL5JW8v1jRSkiOVeKLD7v++NiFYWOSwSa7jVnzhxT6ER6Kti5555b43NooVQb5wEAAEDNaHc1LXIWvFLxsdQSz/5hw0XajGkjsYypa9Wg2aS6HGXePM+9k4LndU2JTpe6/vrrzQjKNddcI2eccYbZZ683Wbx4sYwcOVI6depkChCdAzlgwABZuHBhwHPqc3Ualjc9v+7XNSJ//OMfTRGj2UgdOnSQhx56SMrKyiqs0dHj/YsyXQujN+32l5OTI23btjXnueCCC2TBggVBv8cRI0aYToBNmjQx64c+/PBDc259jXDWEr3++uvSu3dvk/nUqlUrufnmm+Wbb74JeOznn39uugv26NHD/Ex1XUznzp1lwoQJ5vr9f2YffPBB+df2TX9utueff16ysrLM96/n0u9Hm3gsX7485OsHAAAI14DUi2Xm20kBiwV7e8Y7Sea4WMaITpgWLRLJyRHZ5bU2q107kenTRa6N/S575bRF+EUXXSRdu3Y1H67/85//SP369c1j2iJcv77kkkukTZs2sn//fnnjjTdk2LBhpmgZN25cyK/zm9/8xnyg/5//+R/zIf21114zBceJEydk6tSpIZ3j5MmTcsUVV5gC42c/+5l8++238vLLL8vw4cMlPz/fPGbT1uYXX3yx6f535ZVXmo6AW7Zskcsvv1wuu+yysH5G2iJ9zJgxZpHbqFGjpHnz5vLWW2/J4MGDzfXbPy/bokWL5LnnnjNNFbTw02Lu448/lt/97nfmZ6DFVr169cyxOp1QR9x27NjhM7Wwe/fu5V/ffvvtplOhvt6ZZ55pvjf9+em2vpYWQQAAALUt6aNV0vZQ8L/ka7GTerBU5KNVuuhYYpblAIcOHdLOcOY+mO+++87auHGjuY+UhQstKyFBu9T53nSf3vTxWNKhQwfzc/NWWFho9ult0qRJAZ+3devWCvsOHz5sde3a1WrWrJl19OhRn8f0XAMHDvTZN2bMGLP/hz/8ofXVV1+V79+/f7/VvHlzq2nTptbx48fL9y9fvtwcP3ny5IDfQ1ZWls/x7733ntmfmZnpc/x1111n9k+dOtVn/3PPPVf+fetrVUXfaykpKdZpp51mbdmypXz/iRMnrEsvvdScR6/N265du3yu0fbQQw+Z41966SWf/fozq+xXcNu2bRX26c+ybdu21tlnn13l91AXvxMAAMAZTpWespYXLrfm/nuuudftoObOrfiBN9BNj4vR2kAxdS1EOj1NR3ICNeO29+XmOmcaW+vWreW+++4L+NhZZ51VYZ9OAdORH13z849//CPk13nggQfMqJBNmw3oSMThw4fNSEuo/vCHP/iMoPz4xz820+C8r+X48ePy6quvSsuWLeWuu+7yef7YsWPlnHPOCfn1dOREWxdqyK1OP7PpiEywkajU1NQKozzqjjvuMPfvvfeehEPbQvvTn6WOan3xxRdmNAgAAKAqizYtMu2iM17IkOxF2eZet4Nm4Xh9dqtUqMdFCYVOiFas8J2uFqjYKSryHOcEOiUq0IdytW/fPhME26VLF7NGx14/YhcPX331Vciv07Nnzwr72ulcPxE5ePBgSOfQKWOBPvTrebzPoYWTFju9evUy63i86fXrlLZQ/etf/zL3ujbJX79+/SQ5ueKsTx3c0nU1l156qVlPk5SUZF5X1+uE+3NT27ZtM2uCOnbsaNbo2P8dZsyYUa3zAQCA+LOoOsGf+vlHP68Fi8HQ/WlpnuNiGGt0QrR7d+0eF226sD6Qr7/+2iy+37lzp/Tv39+sB9FCQz+0r1+/3izO12IiVIFCnOwioTTE4S9thhCInse7qYGOwCgd0Qnnew5ER66CnUt/Fnbx4u3//b//JzNnzpS0tDT56U9/akZf7IJLGzCE83PTNVTaclu/J13zc/XVV5ufZWJiommmoGt+wjkfAACIz+DPnPwck3uj+TcDdoi0OSKyu4nIig6WWIkJkpufK1nnZPkGfyYleRagDxvmKWq8pzTZxU9enue4GEahEyKXjOCVCxZUqYvptch5+OGH5f777/d57LHHHjOFTqyyiyodkQpk7969IZ/LLq4CnUsLNG3eoFPVbHrcrFmzTDe41atX++QK7dmzxxQ64dCpetp84cUXX5TrrrvO57Fbb721vGMbAABAVcGf1wTNw7Fk8Xme4M9B6X5NBbTLlna4DdSFS4scB3ThYupaiFwyglelrVu3mvtAHb1WxPi8PF2DoyMoa9eurTDaodPKtAAJZ2pfsO9Zz3Pq1KkK08z0NXQEzD88NdjPTUeGgo1sBfvvoK/x0Ucfhfx9AACA+LX78O7yPBzNvwmUh6OPBw3+1GJG40c02mLuXM99YaEjihxFoRMiewRP+Rc7DhrBq5Iu8FcrV6702T937lxZunSpxDItcrQFto7c5Ol/DL9W0Zs3bw75XFpg6AiRrrnRfBzvVtf+I13eP7dVq1b5TKfbtWuXadcdiK7jUUW6uCvE/w46qvbpp5+G/H0AAID41aZxSzOSU1keTl6+57ig9MOttpAeOdJz76APuxQ6YbBH8LxmLBk60qP7HVLcVkrzYnTalmblaE6N5uBoTo3uv9YB3+C0adPMWhwN6bzqqqtMZzktfn71q1+ZXB2l61yqoj8DzQw6evSoWbOkz9cwUB3p0Y5x3p3kvLuhaW6ONkPQn9vo0aPNVDbN8gnEzvXR52nx9Mgjj8ibb75ZPj1NO7zpY9rtThtB6JqpKVOmyJAhQ2rhJwUAANxuwA7PdLVgn3x0f/sSz3FuRKETJoeP4FVJO5np+g9t36ztkJ955hkTjvnOO++YBfGxThsB6NSyn//852Z0RUd2dP2MXn+nTp2CNkgIRMNCFy9eLGeffba88MIL5qbFhv5cAnWs0wBQLUh0bY12RtOiR7vX6WhYINpRTYunAwcOmFBRbcW9cOFC85gWR3rNPXr0MOGgOrKkTSF02poWUgAAIH4bDBRsL5B5G+aZe90OJmlv4HXL1T3OaRI0TEdinHae0r+wayesYB9Sjx07JoWFhaYNsbbiBfxdcsklpgjS95HmArkdvxMAALiLtoLWLmreraLbpbST6VdOl2u7BPire0GBSEZG1SfWv9zrtDSHCKU2UIzowHV2B+jx/dJLL5nREG0WEA9FDgAAcJd4zsOpLtpLw3XOP/98M/XrvPPOK8//0eyZpk2byhNPPBHtywMAAKh2Ho4/3Zcg7s7DqS5GdOA6upBf1+VopzUN8NyyZYtkZ2fLmjVrpGvXrtG+PAAAgGrl4SgN/hxYKPKLDZ573dZip6jEk4cTl920gmBEB64zdepUcwMAAHADO+cmePCnyOLzqsjDycrScD+d4+9JuNfpai4dybFR6AAAAAAxrE3TNuXBn/7s4M9hw0XajPGNvwiYhxNHKHQAAACAGDYg9WLp/LaOvpQGDP7UqPIZ7yRJ65cujtIVxibW6AAAAAAxLOmjVdL2UMUix6b7Uw+WmuPwPUZ0AAAAgDruoqaNA3RNjU5LG9B+gG+3NH8BojNqdFycoNABAAAAYjX0U2nzgFCEelycYOoaAAAAEKuhnyrOgz+ri0IHAAAAqMPQT/8snIQyT5Cnhn7qcRXYwZ/Kv9iJg+DP6qLQAQAAAOoo9FPbRG/PEyl4QWTeQs+9bg/dWEnoZ5wHf1YXhQ5i1oMPPigJCQlSUFAQ7UsBAACoEW08YGfhaPZNoCwcfTxo6KfSYmb7dpHly0XmzvXcFxZS5ARBoeMSWhCEc4uXomTOnDnmuvQeAAAgWto0binT8z1fB8rCUXn5nuMqZQd/jhzpuWe6WlB0XXOJyZMnV9iXl5cnhw4dCvgYAAAA6s6AHSJJfiM5/sVO+xKR1B0i0rEur8y9KHRcQkdU/OkohhY6gR4DAABA3Unau69Wj0PVmLpWDdoNo2B7gczbMM/cB+yOEcNOnDghTz31lPTo0UNOO+00adq0qQwYMEDeeOONCsdqoTRp0iQ577zzpEmTJpKSkiKdOnWSMWPGyI4d+icHHTUdJA899JD5OiMjo3x6XHp6ekjXU1RUJCNHjpTTTz/dvMbAgQPlww8/DHrtM2bMkMzMTElLS5MGDRpIy5Yt5dprr5VPPvnE59jrr79exo4da77W+0BT99auXSt33HGHnH/++dKsWTNp1KiRdO3aVR577DE5efJkGD9VAAAQb8L6TEgWTp1jRKcuQp5iyPHjx+XKK680a2m6d+8uN954o/lAv2TJEsnKyjJFhH7wV5ZlmYLi73//u/Tv3988LzEx0RQ4WhSNGjVKOnToYAoK9cEHH5gCyC5wmjdvXuX17N69W/r16yfFxcXmtbT42rRpk1x++eWmaPL39ddfS25urinMrrrqKvnBD34g27ZtM9fzf//3f6ZA6t27tzl26NChcvDgQXn99dfN96bfr79nn31W3nzzTbn00kvN+b799lvzs5k4caL84x//kIULF9b4Zw4AANwn7M+EdhZOcbF+yKr4uP4hVh8nC6f2WA5w6NAhfTeY+2C+++47a+PGjeY+UhZuXGglPJhgyYPic9N9etPHY0mHDh3Mz83bvffea/Y98MADVllZWfn+kpISq1evXlb9+vWt4uJis+/f//63OXbo0KEVzn3s2DHr8OHD5duTJ082xy5fvjysaxwzZox53iOPPOKz/5lnnjH7/c+pr7tr164K5/n000+tJk2aWIMHD/bZ/5e//MWcQ+8D2bFjh3Xq1CmfffpzueGGG8zzVq5caTlVXfxOAAAQj6r9mXDhQstKSPDcPOWO52bv08dRK7WBYupaNUKe/Nn7goY8xYiysjJ5+umnpWPHjmaqmfcULp2+plPUdGrYokW+qbw6ncufThnTaWY1oa81f/58M/Xsrrvu8nnspptukrPPPjvg66b6948XkR/96EdmBEhHdMKZcta+fXtJ8utWoj+X22+/3Xz93nvvhfEdAQAAt6tR8CdZOHWKqWthhjwFo292O+RpUPogiUVbtmyRb775Rtq2bVu+psbb/v37zf3mzZvNfZcuXeSCCy6QefPmya5du8xUMF2Po1PAdApbbVzPsWPH5LLLLpOGDRv6PKbn1+lyX3zxRYXnrV+/Xn7/+9/LypUrZc+ePRUKmwMHDkibEOe3arE1c+ZMefnll833feTIETNlz/bVV19V+/sDAADuDv7UdtFpXp3UilJEcq60ZPF5lXwm1GImK0tkxQqdw+9Zk6PT1WgTXesodEJUaXhTNY6LBl3foj777DNzC+bo0aPmPjk5Wd5//33TtU3XqtijLmeeeaZZx3PfffdVGA0JhzY6UDqiE0irVq0q7Fu1apUpjNQVV1xhRn10ZElHYV577TX517/+ZdYhhWrYsGFmjU7nzp1lxIgR5lrq1atn1vZMnz49rHMBAID4Cv70Zwd/DhtexWdCOwsHEUWhE6I2TdvU6nHRoB3T1M9+9jNZoMOjITjjjDNMg4I//vGPZsRDCx/d1mweLQh00X51aZcztW9f4DaKe/furbBv6tSppvhYsWKFXHLJJT6Pffzxx6bQCZU2G9AiR5sgaDMG76JNz6WFDgAAQDjBn2X/Df7c9mgVwZ+IONbohGhA+wGmk0aCfL+uxZvuT0tJM8fFKp2KpsXOP//5z7BbJ+uIiT5f1668++67Zp93O2q7SCgtDX2Nko6i6JQ1vR6dwua/nkhHb/xt3brVtKH2L3K0W9q6desqHF/Zdem51JAhQyqMTGkhBQAAECj4U6erJVYR/KnHIboodEKUlJhk2gUq/2LH3s67Ms8cF6t0Ktqvf/1r0x767rvvDljsfPrpp+UjLNu3bze3YCMt3utqtPiwM3FCpY0Fhg8fbl7vySef9Hnsz3/+s3z++ecVnqPtrHWdkffUOy1i9Pux1xh5q+y69FxK1/p403NPmzYt5O8DAADETx4OwZ/OwdS1MGhP9AXDFwTsma5FjhNydLQJgY586FQ0na6l+TG6LkVzbDZs2GCmfq1evdrs00X/GsTZp08fExjaunVrc5yuhdFmAXfeeWf5ee2g0HvvvdcUCjotTXN07EyeYDSYc9myZXL//febguPCCy80OTpLly41a3Deeecdn+PHjRtn9umIjhZJWmxp7o1elzZK0K+9aUaPdo3Ly8szBZKuL1L6evp96e2VV14xeT4XXXSR7Ny504xU6ShPqNP7AABAHOXhEPzpHJYDxEqOju1U6SlreeFya+6/55p73Y5FgXJ0lObGaE5N//79rZSUFKtBgwZW+/btrSuvvNJ6+umnrSNHjpjjioqKrAkTJlgXXXSR1bJlS5Oxo8dde+211urVqyucd86cOVbXrl3N+fR19fVDoVk2I0aMsJo3b241btzYGjBggPXBBx8EzeZZsGCB1aNHD3NsixYtrOHDh1tbt24tz+QpLCz0OX7JkiVW7969rUaNGpVn89j27dtnMnPatm1rNWzY0Fz/rFmzrG3btpnj9JxORY4OAAARyMPR/L127Spm4Xhn4qSleY5DVHN0EvR/JMaVlJSYEQLt0mUvqPenazwKCwvlhz/8YYVWxUA84ncCAIDgdHpa+vR0M5KjeTi6pqbNEZHdTURWdBCxEhPMyE5hTmHFpQmaOThsmOdr74/SdkYhmThRrw0Ua3QAAAAQ13k42/NECl4QmbfQc6/bQzd+n5FYAcGfjsAaHQAAAMSdGufhEPwZ8yh0AAAAEHdqJQ+H4M+YRqEDAACAuKNrcpJKgj9u5+Gkah5Ox7q8MtQW1ugAAAAg7pCH436M6AAAAMAVXdS0cYCuqWnTtI0MaD+g8iB38nBcj0IHAAAA8RX6qbRxgHZJKy72bRHt3SpaH9fjED9T12bNmiXp6ekmm6Nv376yZs2aoMeePHlSpkyZIh07djTHd+vWTfLz/7vyCwAAAKhhkTPslWE+RY4qLik2+/XxoI0Epk/3zb+x2dt5eXRRi6dCZ/78+TJ+/HiZPHmyrFu3zhQumZmZsm9f4PmL999/vzzzzDMyY8YM2bhxo9x6661yzTXXyCeffFIb1w8AAIA4nq6mIzmWWCb0c2ChyC82eO4TyjyjNLn5uea4gMjDcbUEywo0VhecjuD07t1bZs6cabbLysokLS1Nxo0bJxMmTKhwfNu2beW+++6T22+/vXzfz372M2nUqJG89NJLtZZ+Sgo84IvfCQCA2xVsL5CMFzJMHo62ik7z6qJWlCKSc6XI4vNElo9ZLoPSK2kDXVpKHo6DhFIbhL1G58SJE7J27VqZOHFi+b7ExEQZPHiwrF69OuBzjh8/XuFDlhY5K1euDPo6+hy9eX8zAAAAQK2GftrIw3GlsKauHThwQEpLS6VVq1Y++3V7z549AZ+j09qeeuop+eKLL8zoz7vvviuLFi2S3VoxBzFt2jRTpdk3HTECAAAAwgn9lP+GfupxiD8Rz9GZPn26nH322XLuuedK/fr15Y477pCxY8eakaBgdMRIh6LsW1FRUaQvEwAAAA4M/dTpaolVhH7qcYg/YRU6LVq0kKSkJNm7d6/Pft1u3bp1wOeceeaZ8tprr8nRo0dlx44dsnnzZmnSpImcddZZQV+nQYMGZr6d9w3OtX37dklISJDrr7/eZ/+gQYPM/kjRzoB6AwAAzqGNA3TtzbwN88x90EYChH6iNgsdHZHp2bOnLFu2rHyfTkfT7X79+lX6XF2nk5qaKqdOnZKFCxdKVlZWOC+NMIsK75v+d9Ppf9nZ2fLvf/9b3EILJ/3+9HsGAADOp62g06enmwYD2Yuyzb1uB20RTegnajMwVFtLjxkzRnr16iV9+vSRvLw8M1qj09HU6NGjTUGj62zU3//+dykuLpbu3bub+wcffNAUR/fcc0+4L40waG7RddddZ74+cuSIfPzxxzJv3jyzPkoL0/79+0f7EuWvf/2rfPvttxE7v3dBDgAAnJGHo62iA+XhLBi+oGL4J6GfqM1CZ8SIEbJ//36ZNGmSaUCgBYwGgNoNCnbu3Omz/kZb3GqWzrZt28yUtauuukpefPFFad68ebgvjTB06tTJFJXe9L/D1KlTTbvvgoICibb27dtHvNgDAADOy8PRNTVtjojsbiKyooMlVmKCycPJOidLkhKTKoZ+DhvmKWq8ix1CP+NetZoRaEMBXW+jLaB1xEazdWz6AXrOnDnl2wMHDjRBoVrwaNc2/Su+Zus4mvZa10Jh3jzPvW47gGYdqX/84x/mXqd96ToZHWnTkThdZ6VFqncR9OGHH8rVV19t1mfp2iltLKEFU6CRGO3I97vf/c4UWTpVUe91ZE9H8AKpbI3O66+/LldccYWcccYZ5ly61mbUqFHy6aefmsd1+4UXXjBfa06MPU1Pz1nVGh0dgdTAW22Qoec+/fTTZciQIfLRRx9VOFaLRT2v/kzmzp1rCnttj96mTRvJycmR7777rsJzdGqmvu9btmxpzq/vd23BrvsBAEBFK3aukF0lu0yr6O15IgUviMxb6LnX7aEbLSkqKTLHVUDoJ2prRCfuLVokkpMjsmuX7y+S/jXBIb9I3sXFf/7zH7O+Sj/s/+IXvzAFqd384emnnzZBrzr6psWOfnD/5z//aUaFli9fbm66/sd2yy23yPPPP28KD32enktbi69atSqs67vrrrvM8/Sahg4dal5XO++99957Zo3Y+eefL7m5uaag/te//mUKDnuEsKrmA3pNl112maxZs0Z69OhhzqPNNObPny9vv/22md7385//vMLzNCBXRy51bZk+X7/+4x//aIr3v/3tb+XH6c/stttuM4XQNddcYwo1HfnU11u8eLEJywUAALWch6OfwXT9N6Gf8GY5wKFDh3Qc0twH891331kbN2409xGzcKFlJSTooKjvTffpTR+PssLCQvOzyszMrPDYpEmTzGMZGRlmW7/W29ixY61Tp075HPvZZ59ZycnJVrdu3awDBw74PDZt2jTzvCeeeKJ83/Lly80+Pf7IkSPl+3ft2mW1aNHCPDZmzBif8wwcONDs9/bmm2+afV27dq3wuidPnrT27NlTvq3n02P1ew6kQ4cO5ubtoYceMs/55S9/aZWVlZXvX7dunVW/fn2refPmVklJSfn+yZMnm+ObNWtmbd68uXz/t99+a3Xu3NlKTEy0iouLy/f36NHDnGfv3r0Vrsf/+4m0OvmdAACgFiz/8j1rZ4pYpf6fsf570/07UsQcBxwKoTZQEc/RcQ2dnqYjOYEWutn7cnNjZhrbl19+aaZd6e03v/mNXHrppTJlyhQzlUpHZGw6IvP73//etA339swzz5gOeTNmzDCjEt60kYS2DdfRD5tOSVS6duu0004r36+NKXTEJVR/+tOfyvOX/F83OTm5QlhtuHS6W7169eSxxx7zGdm68MILTZONgwcPmnbo/vR7OOecc8q3dfrayJEjzbS8tWvX+hyr59ebP//vBwAAeJCHg0hg6lqodCjUe7paoGJHg031OK91ItGydetWeeihh8zX+qFbCwRtLz1hwgTp2rVr+XE6zUzX3/jTLm1Kp3MF6l6m59RMJJtOIVMDAnQ1CbQvGJ3ipWuBdI1LbSspKTFNMbp06SLtdLqhn4yMDHn22Wdl/fr1Zj2QN50y588+hxZHNp3+p4WgTq/Tn7ee85JLLiELCgCASpCHg0ig0AmVzveszeMiLDMz06wjqUqwEZKvv/7a3HuP/lTm0KFDppFBoKIpnFEYPY+OAnl37qvNQqey69F1Nd7HeQtUqOgIk92EwXb33XebkRtdq/Pkk0/KE088YY7TZgd/+MMfTGEJAEA8dFHTxgG6pqZN0zYyoP0A325p/sjDQQRQ6ITKpb+Awbqe2R/s9UN/06ZNqzxPs2bNzDQuXZyv09q86WL/UGlTAV28r+eq7WLH/p6CXY++rvdx1f153nDDDeamjR5WrFhhpvi98sor8sUXX5jAVv9pggAAuC0PR1tFaxc1W7uUdjL9yukVc3Bs5OEgAlijEyr7FzBIYWD2p6W55hfQbhluT2GrSrdu3cy9frD3F2hfMBpCq23LP/jggyqPtQsG7xGVymgBc9ZZZ5n1S9pS25/dVltbSNcGHdnRrnHa0U07tWmbdX1tAADcHvrpXeR4h37q4wHZeTjK/7MWeTioJgqdUMXZL6C2SNYpV5q9oyGw/nRdyieffFK+ba9p0YYHmlNj04JCGwuESttS24v/7elzNm2O4D0ao+2nlbaeDpU2HDh58qRMnDhR272V79eRFm1XrSNTWpxUlxZL3udV+nr296LNIAAAiIfQz4GFIr/Y4LlPKPP8f6OGfupxAZGHg1rG1LVw2L+AgXJ0tMhx0S+gLqbXDmi//vWvTbexq666Sjp27CiHDx82C/p1xOX666+X2bNnm+N10f3YsWPlL3/5i2l2oBkyOjKjoxkXXXSRvPXWWyG9rr6OrnPRtS0aTqrn0RwdLZi0KYI+ptk3SkdJ9DjN79F8Gu321qFDhwqNBLxpo4AlS5bIiy++KJs2bZIf//jHsm/fPnOdWkhpM4JQpuoFo0WSjhzp96zXokXOu+++a0Zzhg0bZvYBAOD20M/p+Z4uaraiFJGcKy1ZfJ4n9HNQepDGTeThoBZR6IQrjn4Bb775ZjONS8M7P/zwQ3nzzTfNiEf79u3lzjvvNKMj3rRI6Ny5s7nXgE3tSjZ+/HgZPnx4yIWOevzxx02IqZ5jwYIFJuRTGwVoYXP55ZeXH/eTn/zEtMbW19OF/1pUaLe2ygodHVF5//335Xe/+50pbrRBQOPGjc3z7r33XtMhrSamTZtmmkBo9zj9eWnxpQWiNie48cYba3RuAABcHfpp089UMdDBFs6XoGE6EuN0Qbx+wNaOXMEWiuuH4cLCQtPViulBAL8TAIC6VbB1mXTsMdgUNYHWRpSJyK4UkW3r3pNBHX8chSuEW4RSGyhGdAAAAFBjGuaZVDGhoULoZ6qGfnasyytDvKLQAQAAQI3zcAj9RKyh0AEAAEDN83BcmjkI56K9NAAAAGqehxNnmYOIfRQ6AAAAqHkeTpxlDiL2UegAAAAgYB7O9jyRghdE5i303Ov20I2WFJV48nAqIPQTMcR1a3Qc0C0bqBP8LgAAopKHE0eZg4htril0kv77y6OhkY0aNYr25QBRd+rUKXOfnOyaX3MAQB1o07ilTM8PPPUn8b95OHn5ItsebRn8JIR+Iga4ZupavXr1pEGDBiY4iL9kA54wLf0DgP1HAAAAQs3DSQsS+umdh6PHAbHMVX/qbdGihRQXF8uuXbtMWqoWPwnBOn8ALqWF/tGjR02h06ZNG34HAABhIQ8HbuGqQiclJcXcHzhwwBQ8QLzS4qZ58+am4AcAoLQ0jCUz5OHAJVxV6NjFjt50rU6p/lYDcUhHM5myBgBQixaJ5OSI7Nrl2wRNO0EHbIJm5+HoH40DLQfQmQL6OHk4iHGuK3S8P+jpDQAAIJ6LnGHDKtYrWsPo/oAdn+08HD1AixrvJ5OHAwdxTTMCAAAAfE8ntuhITqBBGXtfbq7nuArIw4ELuHZEBwAAIJ7pmhzv6WqBip2iIs9xATtBk4cDh6PQAQAAcCGtTWp8HHk4cDCmrgEAALgQzdMQ7yh0AAAAXMhunhYsTk33p6XRPA3uRaEDAADgQnbzNOVf7NA8DfGAQgcAAMABtDtaQYHIvHme+1DiAmmehnhGMwIAAAC3hX56oXka4lWCZQXqrh5bSkpKpFmzZnLo0CFJSUmJ9uUAAABEPfTTnn7GyAziTUmItQFT1wAAANwY+gnEOQodAAAAF4R+AvBFoQMAAODm0E8gTlHoAAAAxChCP4Hqo9ABAACIUYR+AtVHoQMAABCjeTiEfgLVR6EDAABQh62i09NFMjJEsrM997qt+4Mh9BOoHnJ0AAAAHJCHoyM/hH4CEnJtQKEDAAAQYVqk6MhNsFbRWuzoCE1hIcULUBUCQwEAAGIEeThA3aPQAQAAiDDycIC6R6EDAAAQYeThAHWPQgcAACDCyMMB6h6FDgAAQISRhwPUPQodAACACAd/KvJwgLqVXMevBwAA4IpMnJwc305qWrDoqE1lBYs+lpVFHg5QF8jRAQAAqMPgTwAxnKMza9YsSU9Pl4YNG0rfvn1lzZo1lR6fl5cn55xzjjRq1EjS0tLkzjvvlGPHjlXnpQEAAKJGp6fpSE6gPxPb+3Jzq57GBiDywi505s+fL+PHj5fJkyfLunXrpFu3bpKZmSn79u0LePzcuXNlwoQJ5vhNmzbJc889Z85x77331sb1AwAA1BmCPwEXFzpPPfWU3HzzzTJ27Fg577zzZPbs2dK4cWN5/vnnAx6/atUq6d+/v2RnZ5tRoCuuuEJGjhxZ5SgQAABArCH4E3BpoXPixAlZu3atDB48+PsTJCaa7dWrVwd8zsUXX2yeYxc227Ztk6VLl8pVV10V9HWOHz9u5t553wAAAKKN4E/ApV3XDhw4IKWlpdKqVSuf/bq9efPmgM/RkRx93iWXXCLa9+DUqVNy6623Vjp1bdq0afLQQw+Fc2kAAAB1FvxZXBx4nY42JNDHCf4E4iBHp6CgQB599FH505/+ZNb0LFq0SJYsWSIPP/xw0OdMnDjRdFGwb0U62RUAACDKWTgEfwIuHdFp0aKFJCUlyd69e33263br1q0DPueBBx6QUaNGyU033WS2u3btKkePHpVbbrlF7rvvPjP1zV+DBg3MDQAAIBazcLSFdKDnapFDa2nAgSM69evXl549e8qyZcvK95WVlZntfv36BXzOt99+W6GY0WJJOSDCBwAAuDgLx7+Dmk5J0/36eGW0mNm+XWT5cu0w67kvLKTIARw7oqO0tfSYMWOkV69e0qdPH5ORoyM02oVNjR49WlJTU806G3X11VebTm0XXnihydz58ssvzSiP7rcLHgAAgFjJwtEpaJqFk5VV+RQ0fWzQoIheKoC6LHRGjBgh+/fvl0mTJsmePXuke/fukp+fX96gYOfOnT4jOPfff78kJCSY++LiYjnzzDNNkTN16tSaXDcAAEDEs3AoZADnSrAcMH9M20s3a9bMNCZISUmJ9uUAAAAH08YD2dlVH6dT0kaOrIsrAhCJ2iDiXdcAAABiCVk4QHyg0AEAAHGZhePfHtqm+9PSyMIBnI5CBwAAxBWycID4QKEDAADiLvjTzsJJTfXdryM9up820UAcdl0DAABwS/CntpDW7mq7d3vW5Oh0NUZyAHeg6xoAAHB88Kf/pxl7ChqjM4D70HUNAADEdfCn0uDPqqaxAXAnCh0AAOD64E8A8YdCBwAAOJKuq6nN4wC4C4UOAABwJII/AVSGQgcAADgSwZ8AKkOhAwAAHJmFQ/AngMpQ6AAAgJhoE52eLpKRIZKd7bnXbd1fGYI/AQRDjg4AAHB8Fo6O/hD8CcSHkhBrAwodAAAQNVqg6MhNsDbRWuzo6ExhIYULAA8CQwEAQMwjCwdApFDoAACAqCELB0CkUOgAAICoIQsHQKRQ6AAAgKghCwdApFDoAACAqCELB0CkUOgAAICoBn+ShQMgEpIjclYAABC3mTg5Ob6d1LRg0VGbygoWfSwriywcALWHHB0AABAzwZ8AUBVydAAAQJ3R6Wk6khPoz6f2vtzcqqexAUBtodABAAA1RvAngFhDoQMAAGqM4E8AsYZCBwAA1BjBnwBiDYUOAACoMYI/AcQaCh0AAFBjBH8CiDUUOgAAICCCPwE4GYGhAACgAoI/ATgdgaEAAMAHwZ8AYhmBoQAAIGwEfwJwCwodAABQjuBPAG5BoQMAAMoR/AnALSh0AABAOYI/AbgFhQ4AAChH8CcAt6DQAQDAxcLNwiH4E4BbUOgAAODiNtHp6SIZGSLZ2Z573db9lSH4E4AbkKMDAIAL1UYWjo7+EPwJwKm1AYUOAAAuowWKjtwEaxOtxY6OzhQWUrgAcB4CQwEAiFNk4QAAhQ4AAK5DFg4AUOgAAOA6ZOEAAIUOAACuQxYOAFDoAADgOmThAACFDgAArgz+JAsHQLxLjvYFAACAqjNxcnJ8O6lpwaKjNpUVLPpYVhZZOADiEzk6AAC4PPgTANyEHB0AABxOp6fpSE6gP0na+3Jzq57GBgDxiEIHAIAYRfAnANRxoTNr1ixJT0+Xhg0bSt++fWXNmjVBjx00aJAkJCRUuA0ZMqQGlw0AgPsR/AkAdVjozJ8/X8aPHy+TJ0+WdevWSbdu3SQzM1P27dsX8PhFixbJ7t27y2+ffvqpJCUlyc9//vMaXDYAAO5H8CcA1GEzAh3B6d27t8ycOdNsl5WVSVpamowbN04mTJhQ5fPz8vJk0qRJpug57bTTAh5z/Phxc/NecKSvQTMCAEA80bU36ekixcWB1+loQwLtvlZYSCc1APGjJBLNCE6cOCFr166VwYMHf3+CxESzvXr16pDO8dxzz8kvfvGLoEWOmjZtmrl4+6ZFDgAA8ZaHQ/AnAFRfWIXOgQMHpLS0VFq1auWzX7f37NlT5fN1LY9OXbvpppsqPW7ixImmQrNvRbrSEgAAF7SK1hGajAyR7GzPvW7r/mAI/gQABwSG6mhO165dpU+fPpUe16BBA3MDAMDteTg6LU33V1a0EPwJABEudFq0aGEaCezdu9dnv263bt260ucePXpUXn75ZZkyZUo1LhMAAPfm4eg0NM3D0WImWPGi+wcNivilAkB8Tl2rX7++9OzZU5YtW1a+T5sR6Ha/fv0qfe6rr75qGgxcd9111b9aAAAciDwcAHDA1DVtLT1mzBjp1auXmYKmXdR0tGbs2LHm8dGjR0tqaqppKOA/bW3o0KFyxhln1N7VAwDgAOThAIADCp0RI0bI/v37TYtobUDQvXt3yc/PL29QsHPnTtOJzduWLVtk5cqV8s4779TelQMA4BDk4QCAA3J0YrlXNgAAsYg8HACI8RwdAAAQPvJwAKDuUegAABDB0E8beTgA4OIcHQAA3JCHo62ivbuoabGiIzZVFSvk4QBA3WGNDgAANQz9tKefMTIDAJHHGh0AAOow9FNp6Gco09gAAJFHoQMAQAgI/QQAZ6HQAQAgBIR+AoCzUOgAABACQj8BwFkodAAACIF2R9Puav45ODbdn5bmOQ4AEH0UOgCAuBVOHg6hnwDgLBQ6AIC4bRWdni6SkSGSne25123dHwyhnwDgHOToAADiTk3zcHTkh9BPAIjt2oBCBwAQV7RI0ZGbYK2itdjREZrCQooXAIhFBIYCABAAeTgAEB8odAAAcYU8HACIDxQ6AIC4Qh4OAMQHCh0AQFwhDwcA4gOFDgAgrpCHAwDxgUIHABA3oZ828nAAwP2So30BAADUJA8nJ8e3i5oWKzpiU1Wxoo9nZZGHAwBuRY4OACAuQz8BAM5Ejg4AwLV0epqO5AT6U529Lzc3tGlsAAB3otABADgOoZ8AgKpQ6AAAHIfQTwBAVSh0AACOQ+gnAKAqFDoAAMch9BMAUBUKHQCA4xD6CQCoCoUOAMCRwZ+EfgIAKkNgKADAscGfhH4CAIIhMBQAEFUEfwIAwkFgKAAg5hH8CQCIFAodAEDUEPwJAIgUCh0AQNQQ/AkAiBQKHQBA1BD8CQCIFAodAEDUEPwJAIgUCh0AQNTycAj+BABECoUOAKBWW0Wnp4tkZIhkZ3vudVv3B0PwJwAgEsjRAQDERB6OjvwQ/AkAqK3agEIHAFBjWqToyE2wVtFa7OgITWEhxQsAoGYIDAUA1BnycAAAsYZCBwBQY+ThAABiDYUOAKDGyMMBAMQaCh0AQI2RhwMAiDUUOgCAGiMPBwAQayh0AAA1Cv20kYcDAIglydG+AABA7OXh5OT4dlHTYkVHbKoqVvTxrCzycAAA0UeODgCg1kI/AQCINHJ0AABh0elpOpIT6M9f9r7c3NCmsQEAEG0UOgAAg9BPAICbUOgAAAxCPwEAEu+FzqxZsyQ9PV0aNmwoffv2lTVr1lR6/MGDB+X222+XNm3aSIMGDaRz586ydOnS6l4zACACCP0EAMR1oTN//nwZP368TJ48WdatWyfdunWTzMxM2bdvX8DjT5w4IZdffrls375dFixYIFu2bJFnn31WUv37jwIAoorQTwBAXHdd0xGc3r17y8yZM812WVmZpKWlybhx42TChAkVjp89e7Y8/vjjsnnzZqlXr161LpKuawBQPdo4IJxWz3bXNeX9/w50XQMAuLrrmo7OrF27VgYPHvz9CRITzfbq1asDPueNN96Qfv36malrrVq1kvPPP18effRRKa2kbc/x48fNN+B9AwCER4uW9HSRjAyR7GzPvW7r/mAI/QQAuEVYhc6BAwdMgaIFizfd3rNnT8DnbNu2zUxZ0+fpupwHHnhAnnzySXnkkUeCvs60adNMlWbfdMQIABA6e2TGv4tacbFnf1XFzvbtIsuXi8yd67kvLKTIAQC4eOraV199ZdbWrFq1yozS2O655x754IMP5O9//3uF52jjgWPHjklhYaEk/Xe+xFNPPWWms+0O0rpHR3T0ZtMRHS12mLoGAFXTAXMduQnWKlqnoekIjRYvlU1jAwDAyVPXksM5aYsWLUyxsnfvXp/9ut26deuAz9FOa7o2xy5yVJcuXcwIkE6Fq1+/foXnaGc2vQEAIpuHM2hQXV4ZAAAxOnVNi5KePXvKsmXLyvdpMwLd9h7h8da/f3/58ssvzXG2zz//3BRAgYocAEDNkIcDAEA12ktra2ltD/3CCy/Ipk2b5Ne//rUcPXpUxo4dax4fPXq0TJw4sfx4ffzrr7+WnJwcU+AsWbLENCPQ5gQAgNpHHg4AAGFOXVMjRoyQ/fv3y6RJk8z0s+7du0t+fn55g4KdO3eaTmw2XVvz9ttvy5133ikXXHCBWeOjRc9vf/vb2v1OAAA+eTjaeCDQKkx7jQ55OAAANws7RycayNEBgPCQhwMAcKuI5OgAAKLXSa2gQGTePM99JVFkBnk4AIB4F/bUNQBA3Y/O5OT4dlLTgmX69MoLFn0sK8vTXU0bD+iaHJ2uRktpAEA8YOoaADhgCpr/v9RMQQMAxKsSpq4BgLPp9DQdyQn05yh7X25u1dPYAACIRxQ6AOCC4E8AAOCLQgcAYhTBnwAAVB+FDgDEKII/AQCoPgodAIjx4E+78YA/3Z+WRvAnAACBUOgAQIxm4WgbaG0hrfyLHXs7L4920QAABEKhAwB11CY6PV0kI0MkO9tzr9u6vzIEfwIAUD3k6ACAA7JwdPSH4E8AACTk2oBCBwAiSAsUHbkJ1iZaix0dnSkspHABACAUBIYCQAwgCwcAgOig0AGACCILBwCA6KDQAYAIIgsHAIDooNABgAgiCwcAgOig0AGACCILBwCA6KDQAYAIB3+ShQMAQN1LjsJrAoCjM3Fycnw7qWnBoqM2lRUs+lhWFlk4AADUFXJ0AKAOgz8BAEDNkKMDALVIp6fpSE6gPw3Z+3Jzq57GBgAA6gaFDgCEgOBPAACchUIHAEJA8CcAAM5CoQMAISD4EwAAZ6HQAYAQEPwJAICzUOgAQAgI/gQAwFkodADEpXBDPxXBnwAAOAeBoQDiTnVDPxXBnwAAOAOBoQDiCqGfAAA4G4GhAOCH0E8AAOIHhQ6AuEHoJwAA8YNCB0DcIPQTAID4QaEDIG4Q+gkAQPyg0AEQNwj9BAAgflDoAIibPBxCPwEAiB8UOgAc3So6PV0kI0MkO9tzr9u6PxhCPwEAiA/k6ACIyzwcHfkh9BMAAOcJtTag0AHgOFqk6MhNsFbRWuzoCE1hIcULAABuQ2AoANciDwcAAFSFQgeA45CHAwAAqkKhA8BxyMMBAABVodAB4Djk4QAAgKpQ6ABwHPJwAABAVSh0ADgu+FORhwMAACqTXOmjAFBHmTg5Ob6d1LRg0VGbygoWfSwrizwcAABQETk6ABwd/AkAAOJLCTk6AGKdTk/TkZxAf26x9+XmVj2NDQAAwB+FDoCoIfgTAABECoUOgKgh+BMAAEQKhQ6AqCH4EwAAxFShM2vWLElPT5eGDRtK3759Zc2aNUGPnTNnjiQkJPjc9HkAQPAnAACImUJn/vz5Mn78eJk8ebKsW7dOunXrJpmZmbJv376gz9FuCLt37y6/7dixo6bXDcAFWTgEfwIAgJgpdJ566im5+eabZezYsXLeeefJ7NmzpXHjxvL8888HfY6O4rRu3br81qpVq5peN4AYbBOdni6SkSGSne25123dXxmCPwEAQNQLnRMnTsjatWtl8ODB358gMdFsr169Oujzjhw5Ih06dJC0tDTJysqSzz77rNLXOX78uOmP7X0DEPtZOP4d1IqLPftDKXa2bxdZvlxk7lzPfWEhRQ4AAKijQufAgQNSWlpaYURGt/fs2RPwOeecc44Z7Xn99dflpZdekrKyMrn44otlVyU9ZadNm2ZCgOybFkgA3J2Fo9PTBg0SGTnSc890NQAAENNd1/r16yejR4+W7t27y8CBA2XRokVy5plnyjPPPBP0ORMnTjRJp/atSIM0AMQksnAAAEAsSg7n4BYtWkhSUpLs3bvXZ79u69qbUNSrV08uvPBC+fLLL4Me06BBA3MDEPvIwgEAAI4f0alfv7707NlTli1bVr5Pp6Lpto7chEKnvm3YsEHaEIwBuAJZOAAAwPEjOkpbS48ZM0Z69eolffr0kby8PDl69KjpwqZ0mlpqaqpZZ6OmTJkiF110kXTq1EkOHjwojz/+uGkvfdNNN9X+dwMgalk42ngg0DodbROtj5OFAwAAYrrQGTFihOzfv18mTZpkGhDo2pv8/PzyBgU7d+40ndhs33zzjWlHrcf+4Ac/MCNCq1atMq2pATifnYWj3dW0qPEudsjCAQAA0ZJgWYH+BhtbtL20dl/TxgQaPgogsrRDmjYP0HU1OuVMR2OqKlS0hbR2X/NuTKANE7XIoU00AACo69og7BEdAO4WqGDRqWc6alNZwaKPZWWFXyABAABEAiM6ACoEf/r/q2BPQVuwgNEZAADgjNog4jk6AOIr+BMAACAWUOgAMAj+BAAAbkKhA8Ag+BMAALgJhQ4Ag+BPAADgJhQ6AHyCP+3GA/50v7aLJvgTAAA4AYUO4FLaNKCgQGTePM99VU0E7OBP5V/sEPwJAACchkIHcGmb6PR0kYwMkexsz71u6/7KaOtobSGdmuq7X0d6aC0NAACchBwdwGVqIwtHR38I/gQAAE6uDSh0ABfRAkVHboK1idZiR0dnCgspXAAAgDMRGArEIbJwAAAAPCh0ABchCwcAAMCDQgdwEbJwAAAAPCh0ABchCwcAAMCDQgdwEbJwAAAAPCh0AJcFf5KFAwAAIJIc7QsAUHkmTk6Obyc1LVh01KaygkUfy8oiCwcAAMQvcnQAFwd/AgAAuA05OoCD6fQ0HckJ9GcIe19ubtXT2AAAAOIVhQ4Qgwj+BAAAqBkKHSAGEfwJAABQMxQ6QAwi+BMAAKBmKHSAGETwJwAAQM1Q6AAxiOBPAACAmqHQAeoIwZ8AAAB1h8BQoA4Q/AkAAFC3CAwFIozgTwAAgNpDYCgQAwj+BAAAiA4KHSCCCP4EAACIDgodIIII/gQAAIgOCh0gggj+BAAAiA4KHSCCCP4EAACIDgodIIJZOAR/AgAARAeFDhBGm+j0dJGMDJHsbM+9buv+yhD8CQAAUPfI0QHqKAtHR38I/gQAAKib2oBCB5CqCxQduQnWJlqLHR2dKSykcAEAAIg0AkOBWkIWDgAAgPNQ6ABVIAsHAADAeSh0gCqQhQMAAOA8FDpAFcjCAQAAcB4KHaAKZOEAAAA4D4UO4lK4wZ9k4QAAADhLcrQvAIhGJk5Ojm8nNS1YdNSmsoJFH8vKIgsHAADACcjRQVypjeBPAAAARA85OoAfnZ6mIzmBSnt7X25u1dPYAAAAEPsodBA3CP4EAACIHxQ6iBsEfwIAAMQPCh3EDYI/AQAA4geFDuIGwZ8AAADxo1qFzqxZsyQ9PV0aNmwoffv2lTVr1oT0vJdfflkSEhJk6NCh1XlZoEZ5OAR/AgAAxI+wC5358+fL+PHjZfLkybJu3Trp1q2bZGZmyr59+yp93vbt2+Xuu++WAfy5HLXYKjo9XSQjQyQ723Ov27o/GII/AQAA4kPYOTo6gtO7d2+ZOXOm2S4rK5O0tDQZN26cTJgwIeBzSktL5dJLL5UbbrhBVqxYIQcPHpTXXnst5NckRwe1nYejIz8EfwIAADhPRHJ0Tpw4IWvXrpXBgwd/f4LERLO9evXqoM+bMmWKtGzZUm688caQXuf48ePmG/C+AbWZh6NFzaBBIiNHeu4pcgAAANwlrELnwIEDZnSmVatWPvt1e8+ePQGfs3LlSnnuuefk2WefDfl1pk2bZqo0+6YjRoCNPBwAAABEteva4cOHZdSoUabIadGiRcjPmzhxohmKsm9F+qkV+C/ycAAAAFCVZAmDFitJSUmyd+9en/263bp16wrHb9261TQhuPrqq8v36Zoe88LJybJlyxbp2LFjhec1aNDA3IBAyMMBAABArY7o1K9fX3r27CnLli3zKVx0u1+/fhWOP/fcc2XDhg2yfv368ttPf/pTycjIMF8zJQ3VQR4OAAAAanVER2lr6TFjxkivXr2kT58+kpeXJ0ePHpWxY8eax0ePHi2pqalmnY3m7Jx//vk+z2/evLm5998PhMrOw9Gua1rUeDclIA8HAAAA1Sp0RowYIfv375dJkyaZBgTdu3eX/Pz88gYFO3fuNJ3YgFBVp9WznYej3de8GxPoSI8WOeThAAAAxLewc3SigRwdd+fhBCpWdMQmlGKFPBwAAID4UhJibUChA8eGfgIAACD+lEQiMBSIpdBPAAAAIBgKHUQFoZ8AAACIJAodRAWhnwAAAIgkCh1EBaGfAAAAiCQKHUQFoZ8AAACIJAod1BptHFBQIDJvnue+skYCduin8i92CP0EAABATVHooNZaRaeni2RkiGRne+51W/dXFfqZmuq7X0d6aC0NAACAmiBHB1HPwyH0EwAAAKEiMBR1QosUHbkJ1ipaix0doSkspHgBAABAzREYijpBHg4AAABiEYUOaoQ8HAAAAMQiCh3UCHk4AAAAiEUUOqgR8nAAAAAQiyh0UCPk4QAAACAWUeig2qGfNvJwAAAAEGuSo30BiK08nJwc3y5qWqzoiE1VxYo+npVFHg4AAABiAzk6qJXQTwAAAKAukKODkOn0NB3JCVTy2vtyc0ObxgYAAADEAgodEPoJAAAA16HQAaGfAAAAcB0KHRD6CQAAANeh0AGhnwAAAHAdCh0Q+gkAAADXodBxqXCDPwn9BAAAgJsQGOpC1Q3+JPQTAAAAbkFgqMsQ/AkAAAA3IzA0DhH8CQAAAHhQ6LgIwZ8AAACAB4WOixD8CQAAAHhQ6LgIwZ8AAACAB4WOixD8CQAAAHhQ6LgoD4fgTwAAAMCDQifGW0Wnp4tkZIhkZ3vudVv3B0PwJwAAAECOjmvzcHTkh+BPAAAAuE2otQGFTgzSIkVHboK1itZiR0doCgspXgAAABBfSggMdS7ycAAAAICaodCJQeThAAAAADVDoRODyMMBAAAAaoZCJwaRhwMAAADUDIVODCIPBwAAAKgZCp0YC/20kYcDAAAAVF9yDZ6LEPNwcnJ8u6hpsaIjNlUVK/p4VhZ5OAAAAEC4yNGJ4dBPAAAAAL7I0YkynZ6mIzmBykh7X25uaNPYAAAAAISHQidCCP0EAAAAoodCJ0II/QQAAACih0InQgj9BAAAAKKHQidCCP0EAAAAoodCJ0J5OIR+AgAAANFDoRMibRWdni6SkSGSne25123dHwyhnwAAAICDCp1Zs2ZJenq6NGzYUPr27Str1qwJeuyiRYukV69e0rx5cznttNOke/fu8uKLL4qT2Hk4/l3Uios9+6sqdrZvF1m+XGTuXM99YSFFDgAAABBTgaHz58+X0aNHy+zZs02Rk5eXJ6+++qps2bJFWrZsWeH4goIC+eabb+Tcc8+V+vXry1tvvSV33XWXLFmyRDIzM2M+MFSnp+nITbBW0ToNTUdotHhhGhoAAAAQWaHWBmEXOlrc9O7dW2bOnGm2y8rKJC0tTcaNGycTJkwI6Rw9evSQIUOGyMMPPxzw8ePHj5ub9zejrxGNQkfX4ug0taroSM2gQXVxRQAAAED8Kgmx0Alr6tqJEydk7dq1Mnjw4O9PkJhotlevXl3l87WmWrZsmRn9ufTSS4MeN23aNHPx9k2LnGghDwcAAABwnrAKnQMHDkhpaam0atXKZ79u79mzJ+jztNpq0qSJmbqmIzkzZsyQyy+/POjxEydONM+xb0VFRRIt5OEAAAAAzpNcFy/StGlTWb9+vRw5csSM6IwfP17OOussGRRkrleDBg3MLZbycLTxQKBJfvYaHfJwAAAAAIcWOi1atJCkpCTZu3evz37dbt26ddDn6fS2Tp06ma+169qmTZvM9LRghU4ssfNwtLuaFjXexQ55OAAAAIALpq7p1LOePXuaURmbNiPQ7X79+oV8Hn2Od7OBWEceDgAAAODyqWs67WzMmDEmG6dPnz6mvfTRo0dl7Nix5nFtPZ2ammpGbJTe67EdO3Y0xc3SpUtNjs7TTz8tTqLFTFaWyIoVnsYDuiZHp6sxkgMAAADEnrALnREjRsj+/ftl0qRJpgGBTkXLz88vb1Cwc+dOM1XNpkXQbbfdJrt27ZJGjRqZPJ2XXnrJnMdptKhxwGw7AAAAIO6FnaMTDdEMDAUAAADg8hwdAAAAAHACCh0AAAAArkOhAwAAAMB1KHQAAAAAuA6FDgAAAADXodABAAAA4DoUOgAAAABch0IHAAAAgOtQ6AAAAABwHQodAAAAAK5DoQMAAADAdSh0AAAAALhOsjiAZVnmvqSkJNqXAgAAACCK7JrArhEcXegcPnzY3KelpUX7UgAAAADESI3QrFmzoI8nWFWVQjGgrKxMvvrqK2natKkkJCREvYLUgquoqEhSUlKiei1wHt4/qAneP6gu3juoCd4/iLX3j5YvWuS0bdtWEhMTnT2io99Au3btJJbofyh+2VFdvH9QE7x/UF28d1ATvH8QS++fykZybDQjAAAAAOA6FDoAAAAAXIdCJ0wNGjSQyZMnm3sgXLx/UBO8f1BdvHdQE7x/4NT3jyOaEQAAAABAOBjRAQAAAOA6FDoAAAAAXIdCBwAAAIDrUOgAAAAAcB0KHQAAAACuQ6ETwKxZsyQ9PV0aNmwoffv2lTVr1lR6/KuvvirnnnuuOb5r166ydOnSOrtWOPv98+yzz8qAAQPkBz/4gbkNHjy4yvcb3Cvcf3tsL7/8siQkJMjQoUMjfo1wz/vn4MGDcvvtt0ubNm1M29fOnTvz/19xLNz3T15enpxzzjnSqFEjSUtLkzvvvFOOHTtWZ9eL2PDhhx/K1VdfLW3btjX/P/Taa69V+ZyCggLp0aOH+XenU6dOMmfOnIhdH4WOn/nz58v48eNNv+9169ZJt27dJDMzU/bt2xfw+FWrVsnIkSPlxhtvlE8++cR80NDbp59+WufXDue9f/SXXd8/y5cvl9WrV5v/s7jiiiukuLi4zq8dznrv2LZv3y533323KZgRv8J9/5w4cUIuv/xy8/5ZsGCBbNmyxfzhJTU1tc6vHc57/8ydO1cmTJhgjt+0aZM899xz5hz33ntvnV87ouvo0aPm/aKFcigKCwtlyJAhkpGRIevXr5fc3Fy56aab5O23347MBWqODr7Xp08f6/bbby/fLi0ttdq2bWtNmzYt4PHDhw+3hgwZ4rOvb9++1q9+9auIXyuc//7xd+rUKatp06bWCy+8EMGrhFveO/p+ufjii60///nP1pgxY6ysrKw6ulo4/f3z9NNPW2eddZZ14sSJOrxKxKpw3z967GWXXeazb/z48Vb//v0jfq2IXSJiLV68uNJj7rnnHutHP/qRz74RI0ZYmZmZEbkmRnT8/sK1du1aM33IlpiYaLb1r+2B6H7v45X+FSTY8XCv6rx//H377bdy8uRJOf300yN4pXDLe2fKlCnSsmVLM6KM+FWd988bb7wh/fr1M1PXWrVqJeeff748+uijUlpaWodXDqe+fy6++GLzHHt627Zt28y0x6uuuqrOrhvOtLqOPzcnR+SsDnXgwAHzj7z+o+9Ntzdv3hzwOXv27Al4vO5HfKnO+8ffb3/7WzPP1f8fAbhbdd47K1euNNNFdOgf8a067x/9YPr+++/LL3/5S/MB9csvv5TbbrvN/KFFpyMhflTn/ZOdnW2ed8kll+jMIDl16pTceuutTF1DlYJ9bi4pKZHvvvvOrPmqTYzoADHiscceM4vKFy9ebBaDAsEcPnxYRo0aZdZUtGjRItqXAwcqKyszo4H/+7//Kz179pQRI0bIfffdJ7Nnz472pcEBdH2pjgD+6U9/Mmt6Fi1aJEuWLJGHH3442pcG+GBEx4t+YEhKSpK9e/f67Nft1q1bB3yO7g/neLhXdd4/tieeeMIUOu+9955ccMEFEb5SOP29s3XrVrOIXDvdeH9wVcnJyWZheceOHevgyuHUf3u001q9evXM82xdunQxf23VqUz169eP+HXDue+fBx54wPyxRReRK+04q4vSb7nlFlMw69Q3IJzPzSkpKbU+mqN4J3rRf9j1L1vLli3z+fCg2zqXORDd7328evfdd4MeD/eqzvtH/f73vzd/BcvPz5devXrV0dXCye8dbWe/YcMGM23Nvv30pz8t72Kj3fsQP6rzb0///v3NdDW7QFaff/65KYAocuJLdd4/up7Uv5ixi2bPmnRAYuNzc0RaHDjYyy+/bDVo0MCaM2eOtXHjRuuWW26xmjdvbu3Zs8c8PmrUKGvChAnlx3/00UdWcnKy9cQTT1ibNm2yJk+ebNWrV8/asGFDFL8LOOX989hjj1n169e3FixYYO3evbv8dvjw4Sh+F3DCe8cfXdfiW7jvn507d5oOj3fccYe1ZcsW66233rJatmxpPfLII1H8LuCU949+1tH3z7x586xt27ZZ77zzjtWxY0fTiRbx5fDhw9Ynn3xiblpWPPXUU+brHTt2mMf1faPvH5u+Xxo3bmz95je/MZ+bZ82aZSUlJVn5+fkRuT4KnQBmzJhhtW/f3nwA1ZaLH3/8cfljAwcONB8ovL3yyitW586dzfHaMm/JkiVRuGo48f3ToUMH8w+D/03/TwTxJ9x/e7xR6CDc98+qVatMHIJ+wNVW01OnTjUtyxGfwnn/nDx50nrwwQdNcdOwYUMrLS3Nuu2226xvvvkmSlePaFm+fHnAzzH2+0Xv9f3j/5zu3bub95r+2/OXv/wlYteXoP8TmbEiAAAAAIgO1ugAAAAAcB0KHQAAAACuQ6EDAAAAwHUodAAAAAC4DoUOAAAAANeh0AEAAADgOhQ6AAAAAFyHQgcAAACA61DoAAAAAHAdCh0AAAAArkOhAwAAAEDc5v8DgAUxBoZzlyQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "    \n",
    "plot_predictions(predictions = y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "    \n",
    "plot_predictions(predictions = y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
